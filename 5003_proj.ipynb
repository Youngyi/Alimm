{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from gensim.models import Word2Vec\n",
    "############pyspark#############\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time2cov(value):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_hour(x):\n",
    "    if (7<=x<=12):\n",
    "        return 1\n",
    "    elif (13 <= x<=20):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slide_cnt(data):\n",
    "\n",
    "    print('当前日期前一天的cnt')\n",
    "    for d in range(19, 26):  # 18到24号\n",
    "        df1 = data[data['day'] == d - 1]\n",
    "        df2 = data[data['day'] == d]  # 19到25号\n",
    "        user_cnt = df1.groupby(by='user_id').count()['instance_id'].to_dict()\n",
    "        item_cnt = df1.groupby(by='item_id').count()['instance_id'].to_dict()\n",
    "        shop_cnt = df1.groupby(by='shop_id').count()['instance_id'].to_dict()\n",
    "        df2['user_cnt1'] = df2['user_id'].apply(lambda x: user_cnt.get(x, 0))\n",
    "        df2['item_cnt1'] = df2['item_id'].apply(lambda x: item_cnt.get(x, 0))\n",
    "        df2['shop_cnt1'] = df2['shop_id'].apply(lambda x: shop_cnt.get(x, 0))\n",
    "        df2 = df2[['user_cnt1', 'item_cnt1', 'shop_cnt1', 'instance_id']]\n",
    "        if d == 19:\n",
    "            Df2 = df2\n",
    "        else:\n",
    "            Df2 = pd.concat([df2, Df2])\n",
    "    data = pd.merge(data, Df2, on=['instance_id'], how='left')\n",
    "    print('当前日期之前的cnt')\n",
    "    for d in range(19, 26):\n",
    "        # 19到25，25是test\n",
    "        df1 = data[data['day'] < d]\n",
    "        df2 = data[data['day'] == d]\n",
    "        user_cnt = df1.groupby(by='user_id').count()['instance_id'].to_dict()\n",
    "        item_cnt = df1.groupby(by='item_id').count()['instance_id'].to_dict()\n",
    "        shop_cnt = df1.groupby(by='shop_id').count()['instance_id'].to_dict()\n",
    "        df2['user_cntx'] = df2['user_id'].apply(lambda x: user_cnt.get(x, 0))\n",
    "        df2['item_cntx'] = df2['item_id'].apply(lambda x: item_cnt.get(x, 0))\n",
    "        df2['shop_cntx'] = df2['shop_id'].apply(lambda x: shop_cnt.get(x, 0))\n",
    "        df2 = df2[['user_cntx', 'item_cntx', 'shop_cntx', 'instance_id']]\n",
    "        if d == 19:\n",
    "            Df2 = df2\n",
    "        else:\n",
    "            Df2 = pd.concat([df2, Df2])\n",
    "    data = pd.merge(data, Df2, on=['instance_id'], how='left')\n",
    "\n",
    "    print(\"前一个小时的统计量\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_data(data):\n",
    "    data['time'] = data.context_timestamp.apply(time2cov)\n",
    "    data['day'] = data.time.apply(lambda x: int(x[8:10]))\n",
    "    data['hour'] = data.time.apply(lambda x: int(x[11:13]))\n",
    "    user_query_day = data.groupby(['user_id', 'day']).size(\n",
    "    ).reset_index().rename(columns={0: 'user_query_day'})\n",
    "    data = pd.merge(data, user_query_day, 'left', on=['user_id', 'day'])\n",
    "    user_query_day_hour = data.groupby(['user_id', 'day', 'hour']).size().reset_index().rename(\n",
    "        columns={0: 'user_query_day_hour'})\n",
    "    data = pd.merge(data, user_query_day_hour, 'left',\n",
    "                    on=['user_id', 'day', 'hour'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataClean(raw_data):\n",
    "    print(\"###############  Cleaning & Encoding categorical data to one-hot expression   ################\")\n",
    "    le = LabelEncoder()\n",
    "    print(\"###############  item  ###############\")\n",
    "    for i in range(3):\n",
    "        split = udf(lambda x: x.split(\";\")[i] if len(x.split(\";\")) > i else \" \", StringType())\n",
    "        raw_data = raw_data.select('*', split(raw_data['item_category_list']).alias('category_%d' % (i)))\n",
    "    raw_data = raw_data.drop('item_category_list')\n",
    "   \n",
    "    for i in range(3):\n",
    "        raw_data = raw_data.select('*', split(raw_data['item_property_list']).alias('property_%d' % (i)))\n",
    "    raw_data = raw_data.drop('item_property_list')\n",
    "    \n",
    "    for col in ['item_id', 'item_brand_id', 'item_city_id']:\n",
    "        string = udf(lambda s: s, StringType())\n",
    "        raw_data.select(string(col))\n",
    "        \n",
    "    time_tran = udf(lambda t: time2cov(t))\n",
    "    #raw_data['realtime'] = raw_data['context_timestamp'].apply(time2cov)\n",
    "    raw_data = raw_data.select('*', time_tran(raw_data['context_timestamp']).alias('realtime'))\n",
    "    \n",
    "    for i in range(3):\n",
    "        split_1 = udf(lambda y: y.split(\";\")[i].split(\":\")[0] if len(y.split(\";\")) > i else \" \", StringType())\n",
    "        raw_data = raw_data.select('*', split_1(raw_data['predict_category_property']).alias('predict_category_%d' % (i)))\n",
    "    #raw_data.printSchema()\n",
    "    \n",
    "    print(\"###############  user  ###############\")\n",
    "    for col in ['user_id']:\n",
    "        #raw_data[col] = le.fit_transform(raw_data[col])\n",
    "        encode = udf(lambda l: le.fit_transform(l))\n",
    "        raw_data = raw_data.select('*', encode(raw_data[col]))\n",
    "    print('user 0,1 feature')\n",
    "    \n",
    "    mapping_g = udf(lambda g: 1 if g == -1 else 2)\n",
    "    mapping_a = udf(lambda a: 1 if a == 1004 | a == 1005 | a == 1006 | a == 1007  else 2)\n",
    "    mapping_o = udf(lambda o: 1 if o == -1 | o == 2003  else 2)\n",
    "    mapping_s = udf(lambda s: 1 if s == -1 | s == 3000 | s == 3001  else 2)\n",
    "    raw_data = raw_data.select('*', mapping_g(raw_data['user_gender_id']).alias('gender0'))\n",
    "    raw_data = raw_data.select('*', mapping_a(raw_data['user_age_level']).alias('age0'))\n",
    "    raw_data = raw_data.select('*', mapping_o(raw_data['user_occupation_id']).alias('occupation0'))\n",
    "    raw_data = raw_data.select('*', mapping_s(raw_data['user_star_level']).alias('star0'))\n",
    "    #raw_data.printSchema()\n",
    "    \n",
    "    print(\"###############  context  ###############\")\n",
    "    time_convert = udf(lambda t: pd.to_datetime(t))\n",
    "    raw_data = raw_data.select('*', time_convert(raw_data['realtime']))\n",
    "    raw_data.printSchema()\n",
    "    mapping_l = udf(lambda l: len(str(l).split(';')))\n",
    "    raw_data = raw_data.select('*', mapping_l(raw_data['predict_category_property']).alias('len_predict_category_property'))\n",
    "                    \n",
    "    print('context 0,1 feature')\n",
    "    mapping_c = udf(lambda c: 1 if c == 4001 | c == 4002 | c == 4003 | c == 4004 | c == 4007  else 2)\n",
    "    raw_data = raw_data.select('*', mapping_c(raw_data['context_page_id']).alias('context_page0'))\n",
    "    \n",
    "\n",
    "    print(\"###############  shop  ###############\")\n",
    "    mapping_shop = udf(lambda s: 0 if s <= 0.98 and s >= 0.96  else 1)\n",
    "    for col in ['shop_id']:\n",
    "        raw_data = raw_data.select('*', encode(raw_data[col]))\n",
    "    \n",
    "    raw_data = raw_data.select('*', mapping_shop(raw_data['shop_score_delivery']).alias('shop_score_delivery0'))\n",
    "    raw_data.printSchema()\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#归一化(价格与最低价差值占总价格的百分比)\n",
    "def price_normalization(df):\n",
    "    #df['item_norm_price_score'] = (df['item_price_level'] - df['item_price_level'].min()) / df['item_price_level'].mean()\n",
    "    df.describe('item_price_level').show()\n",
    "    #df1.select('min(item_price_level)')\n",
    "    #df = df.select('*',((df['item_price_level']-df1['min(item_price_level)'])/df1['avg(item_price_level)']).alias('item_norm_price_score'))\n",
    "    \n",
    "    \n",
    "    #df = df.withColumn(\"min\", df1['min(item_price_level)'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #df1.select('min(item_price_level)', 'avg(item_price_level)').show()\n",
    "    #df = df.drop('item_price_level')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureExtraction(raw_data):\n",
    "    print(\"###############  Extracting Training Features   ################\")\n",
    "    #user action series\n",
    "    raw_data = raw_data.orderBy('realtime')\n",
    "    print raw_data.columns\n",
    "    # 商品的价格在同类商品中的比值\n",
    "    print('一个item在同类商品中item_price_level的归一化分数…(在同类商品价格中的分布)(查询词的预测cat0_1_2相同 if( != ' '))')\n",
    "    \n",
    "    raw_data_g = raw_data.groupby('predict_category_0', 'predict_category_1', 'predict_category_2').count().alias(\"counts\")\n",
    "    raw_data_g = raw_data_g.drop('counts')\n",
    "    raw_data_joined = raw_data_g.join(raw_data, ['predict_category_0', 'predict_category_1', 'predict_category_2'])\n",
    "    item_norm_price_score = price_normalization(raw_data_joined)\n",
    "    #raw_data_g = raw_data.groupby('predict_category_0', 'predict_category_1', 'predict_category_2').agg(min('item_price_level'), avg('item_price_level')).na.fill(0)\n",
    "    #raw_data_g.select('min(item_price_level)', 'avg(item_price_level)')\n",
    "    print(len(item_norm_price_score))\n",
    "    print(len(raw_data))\n",
    "    #raw_data['item_norm_price_score'] = item_norm_price_score['item_norm_price_score']\n",
    "    raw_data = raw_data.join(item_norm_price_score)\n",
    "    raw_data.printSchema()\n",
    "    print('一个user有多少item_id,item_brand_id……')\n",
    "    user_cnt = raw_data.groupby(['user_id'], as_index=False)['instance_id'].agg({'user_cnt': 'count'})\n",
    "    raw_data = pd.merge(raw_data, user_cnt, on=['user_id'], how='left')\n",
    "    for col in ['item_id',\n",
    "                'item_brand_id','item_city_id','item_price_level',\n",
    "                'item_sales_level','item_collected_level','item_pv_level']:\n",
    "        item_shop_cnt = raw_data.groupby([col, 'user_id'], as_index=False)['instance_id'].agg({str(col)+'_user_cnt': 'count'})\n",
    "        raw_data = pd.merge(raw_data, item_shop_cnt, on=[col, 'user_id'], how='left')\n",
    "        raw_data[str(col) + '_user_prob'] = raw_data[str(col) + '_user_cnt'] / raw_data['user_cnt']\n",
    "    del raw_data['user_cnt']\n",
    "\n",
    "    print('一个shop有多少item_id,item_brand_id,item_city_id,item_price_level……')\n",
    "    itemcnt = raw_data.groupby(['shop_id'], as_index=False)['instance_id'].agg({'shop_cnt': 'count'})\n",
    "    raw_data = pd.merge(raw_data, itemcnt, on=['shop_id'], how='left')\n",
    "    for col in ['item_id',\n",
    "                'item_brand_id','item_city_id','item_price_level',\n",
    "                'item_sales_level','item_collected_level','item_pv_level']:\n",
    "        item_shop_cnt = raw_data.groupby([col, 'shop_id'], as_index=False)['instance_id'].agg({str(col)+'_shop_cnt': 'count'})\n",
    "        raw_data = pd.merge(raw_data, item_shop_cnt, on=[col, 'shop_id'], how='left')\n",
    "        raw_data[str(col) + '_shop_prob'] = raw_data[str(col) + '_shop_cnt'] / raw_data['shop_cnt']\n",
    "    del raw_data['shop_cnt']\n",
    "\n",
    "\n",
    "    print('一个brand的平均价格指数（平均值，方差，衡量品牌的类型）…')\n",
    "\n",
    "    brand_price_mean = raw_data.groupby('item_brand_id', as_index=False)['item_price_level'].agg({'brand_price_mean': 'mean'})\n",
    "    brand_price_std = raw_data.groupby('item_brand_id', as_index=False)['item_price_level'].agg({'brand_price_std': 'std'})\n",
    "    raw_data = pd.merge(raw_data, brand_price_mean, on=['item_brand_id'], how='left')\n",
    "    raw_data = pd.merge(raw_data, brand_price_std, on=['item_brand_id'], how='left')\n",
    "\n",
    "\n",
    "    print(\"###############  Constructing user_items_shop feature series  ################\")\n",
    "    # 用每个用户的浏览（购买）序列来编码 每个商品的embedding vector，以及每个店铺的embedding vector\n",
    "    # 1. 构造每个用户的购买序列data frame (过滤掉浏览记录过少的用户)\n",
    "    user_list = raw_data['user_id'].unique()\n",
    "    item_list = raw_data['item_id'].unique()\n",
    "\n",
    "    seq_item_id = [[] for i in range(len(user_list))]\n",
    "    seq_item_brand_id = [[] for i in range(len(user_list))]\n",
    "    seq_shop_id = [[] for i in range(len(user_list))]\n",
    "\n",
    "    #seq_item_category_0 = [[] for i in range(len(user_list))]\n",
    "    seq_item_category_1 = [[] for i in range(len(user_list))]\n",
    "    #seq_item_category_2 = [[] for i in range(len(user_list))]\n",
    "\n",
    "    seq_item_property_0 = [[] for i in range(len(user_list))]\n",
    "    seq_item_property_1 = [[] for i in range(len(user_list))]\n",
    "    seq_item_property_2 = [[] for i in range(len(user_list))]\n",
    "\n",
    "\n",
    "    for i in range(len(user_list)):\n",
    "        user_instance_series = raw_data.loc[raw_data['user_id'] == user_list[i]]\n",
    "        seq_item_id[i] = user_instance_series['item_id'].tolist()\n",
    "        seq_item_brand_id[i] = user_instance_series['item_brand_id'].tolist()\n",
    "        #seq_shop_id[i] = user_instance_series['shop_id'].tolist()\n",
    "        #seq_item_category_0[i] = user_instance_series['category_0'].tolist()\n",
    "        seq_item_category_1[i] = user_instance_series['category_1'].tolist()\n",
    "        #seq_item_category_2[i] = user_instance_series['category_2'].tolist()\n",
    "\n",
    "        seq_item_property_0[i] = user_instance_series['property_0'].tolist()\n",
    "        seq_item_property_1[i] = user_instance_series['property_1'].tolist()\n",
    "        seq_item_property_2[i] = user_instance_series['property_2'].tolist()\n",
    "        if(i%5000 == 0):\n",
    "            print('%.2f%%' % (i/len(user_list) * 100))\n",
    "\n",
    "\n",
    "    #train the embedding layer : output item2vec dense features\n",
    "    print(\"###############  Training the Embedding vector of user items series  ################\")\n",
    "    spare_features = {'item_id','item_brand_id','category_1','property_0','property_1','property_2'}\n",
    "    seq_data = {'item_id':seq_item_id,'item_brand_id':seq_item_brand_id,'category_1':seq_item_category_1,'property_0':seq_item_property_0,'property_1':seq_item_property_1,'property_2':seq_item_property_2}\n",
    "    for feat in spare_features:\n",
    "        if(feat == \"category_1\"):\n",
    "            # raw_data[feat + '_vec'] = item2vecTraining2(feat, seq_data, raw_data[feat].unique(), size=5, window=3, min_count=1, workers=1, iter=3, sample=1e-4,negative=20)\n",
    "            raw_data=raw_data.merge(item2vecTraining2(feat, seq_data, raw_data[feat].unique(), size=5, window=3, min_count=1, workers=1, iter=5, sample=1e-4,negative=20), left_on=feat,right_index=True,how='left')\n",
    "#iter=3 min_count=1\n",
    "        else:\n",
    "            # raw_data[feat + '_vec'] = item2vecTraining2(feat, seq_data, raw_data[feat].unique(), size=50, window=3, min_count=1, workers=1, iter=3, sample=1e-4,negative=20)\n",
    "            raw_data=raw_data.merge(item2vecTraining2(feat, seq_data, raw_data[feat].unique(), size=50, window=3, min_count=1, workers=1, iter=8, sample=1e-4,negative=20), left_on=feat,right_index=True,how='left')\n",
    "    return raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def item2vecTraining(feat,seq_data,item_list,**params):\n",
    "    model = Word2Vec(seq_data[feat], **params)\n",
    "    # 训练skip-gram模型; 默认window=5\n",
    "    weights = model.wv.syn0\n",
    "    vocab = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n",
    "    np.save(\"./embedded_weight/\"+feat+\".npy\", weights)\n",
    "    np.save(\"./embedded_weight/vocab_map/\"+feat+\"_map.npy\",vocab)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def item2vecTraining2(feat,seq_data,key_list,**params):\n",
    "    model = Word2Vec(seq_data[feat], **params)\n",
    "    # 训练skip-gram模型; 默认window=5\n",
    "    item_vec_dict = {k : model[k] for k in key_list}\n",
    "    item_vec_series = pd.Series(item_vec_dict)\n",
    "    print(item_vec_series)\n",
    "    #del model\n",
    "    return item_vec_series.to_frame(name=feat + '_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readData():\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    train_data = sc.textFile(\"./data/train.txt\")\n",
    "    test_data = sc.textFile(\"./data/test.txt\")\n",
    "    train_pairs = train_data.map(lambda m: tuple(m.split(' '))).filter(lambda x:len(x)==27)\n",
    "    train_pairs.cache()\n",
    "    test_pairs = test_data.map(lambda m: tuple(m.split(' ')))\n",
    "    test_pairs.cache()\n",
    "    # Create the DataFrame from an RDD of tuples\n",
    "    train_header = ['instance_id', 'item_id', 'item_category_list', 'item_property_list', 'item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level', 'user_id', 'user_gender_id', 'user_age_level', 'user_occupation_id', 'user_star_level', 'context_id', 'context_timestamp', 'context_page_id', 'predict_category_property', 'shop_id', 'shop_review_num_level', 'shop_review_positive_rate', 'shop_star_level', u'shop_score_service', 'shop_score_delivery', 'shop_score_description', 'is_trade']\n",
    "    test_header = test_pairs.collect()[0]\n",
    "    train_df = spark.createDataFrame(train_pairs, train_header)\n",
    "    test_df = spark.createDataFrame(test_pairs, test_header)\n",
    "    train_df = train_df.filter(\"instance_id!='instance_id'\")  \n",
    "    test_df = test_df.filter(\"instance_id!='instance_id'\")  \n",
    "    train_data = dataClean(train_df)\n",
    "    test_data = dataClean(test_df)\n",
    "    \n",
    "    #all_data = featureExtraction(pd.concat([train_data, test_data]).reset_index(drop=True))\n",
    "    all_data = train_data.union(test_data.withColumn(\"is_trade\", lit('null')))\n",
    "    all_data = featureExtraction(all_data)\n",
    "    \n",
    "    \n",
    "    #print(type(all_data['item_id'].unique()))\n",
    "\n",
    "    #np.save(\"./data/all_data.npy\",all_data)\n",
    "    #all_data.to_pickle('./data/all_data.pkl')\n",
    "    # print(train_data.groupby(['predict_category_0', 'predict_category_1','predict_category_2'], as_index=False)['item_price_level'].apply(price_normalization))\n",
    "    # print(all_data.groupby(['predict_category_0', 'predict_category_1', 'predict_category_2'], as_index=False)[\n",
    "    #               'item_price_level'].apply(price_normalization))\n",
    "    # print(sum(train_data.groupby(['predict_category_0', 'predict_category_1','predict_category_2'], as_index=False)['item_price_level'].apply(price_normalization) != 0))\n",
    "    # print(train_data)\n",
    "\n",
    "    return  train_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############  Cleaning & Encoding categorical data to one-hot expression   ################\n",
      "###############  item  ###############\n",
      "###############  user  ###############\n",
      "user 0,1 feature\n",
      "###############  context  ###############\n",
      "root\n",
      " |-- instance_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- item_brand_id: string (nullable = true)\n",
      " |-- item_city_id: string (nullable = true)\n",
      " |-- item_price_level: string (nullable = true)\n",
      " |-- item_sales_level: string (nullable = true)\n",
      " |-- item_collected_level: string (nullable = true)\n",
      " |-- item_pv_level: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_gender_id: string (nullable = true)\n",
      " |-- user_age_level: string (nullable = true)\n",
      " |-- user_occupation_id: string (nullable = true)\n",
      " |-- user_star_level: string (nullable = true)\n",
      " |-- context_id: string (nullable = true)\n",
      " |-- context_timestamp: string (nullable = true)\n",
      " |-- context_page_id: string (nullable = true)\n",
      " |-- predict_category_property: string (nullable = true)\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- shop_review_num_level: string (nullable = true)\n",
      " |-- shop_review_positive_rate: string (nullable = true)\n",
      " |-- shop_star_level: string (nullable = true)\n",
      " |-- shop_score_service: string (nullable = true)\n",
      " |-- shop_score_delivery: string (nullable = true)\n",
      " |-- shop_score_description: string (nullable = true)\n",
      " |-- is_trade: string (nullable = true)\n",
      " |-- category_0: string (nullable = true)\n",
      " |-- category_1: string (nullable = true)\n",
      " |-- category_2: string (nullable = true)\n",
      " |-- property_0: string (nullable = true)\n",
      " |-- property_1: string (nullable = true)\n",
      " |-- property_2: string (nullable = true)\n",
      " |-- realtime: string (nullable = true)\n",
      " |-- predict_category_0: string (nullable = true)\n",
      " |-- predict_category_1: string (nullable = true)\n",
      " |-- predict_category_2: string (nullable = true)\n",
      " |-- <lambda>(user_id): string (nullable = true)\n",
      " |-- gender0: string (nullable = true)\n",
      " |-- age0: string (nullable = true)\n",
      " |-- occupation0: string (nullable = true)\n",
      " |-- star0: string (nullable = true)\n",
      " |-- <lambda>(realtime): string (nullable = true)\n",
      "\n",
      "context 0,1 feature\n",
      "###############  shop  ###############\n",
      "root\n",
      " |-- instance_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- item_brand_id: string (nullable = true)\n",
      " |-- item_city_id: string (nullable = true)\n",
      " |-- item_price_level: string (nullable = true)\n",
      " |-- item_sales_level: string (nullable = true)\n",
      " |-- item_collected_level: string (nullable = true)\n",
      " |-- item_pv_level: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_gender_id: string (nullable = true)\n",
      " |-- user_age_level: string (nullable = true)\n",
      " |-- user_occupation_id: string (nullable = true)\n",
      " |-- user_star_level: string (nullable = true)\n",
      " |-- context_id: string (nullable = true)\n",
      " |-- context_timestamp: string (nullable = true)\n",
      " |-- context_page_id: string (nullable = true)\n",
      " |-- predict_category_property: string (nullable = true)\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- shop_review_num_level: string (nullable = true)\n",
      " |-- shop_review_positive_rate: string (nullable = true)\n",
      " |-- shop_star_level: string (nullable = true)\n",
      " |-- shop_score_service: string (nullable = true)\n",
      " |-- shop_score_delivery: string (nullable = true)\n",
      " |-- shop_score_description: string (nullable = true)\n",
      " |-- is_trade: string (nullable = true)\n",
      " |-- category_0: string (nullable = true)\n",
      " |-- category_1: string (nullable = true)\n",
      " |-- category_2: string (nullable = true)\n",
      " |-- property_0: string (nullable = true)\n",
      " |-- property_1: string (nullable = true)\n",
      " |-- property_2: string (nullable = true)\n",
      " |-- realtime: string (nullable = true)\n",
      " |-- predict_category_0: string (nullable = true)\n",
      " |-- predict_category_1: string (nullable = true)\n",
      " |-- predict_category_2: string (nullable = true)\n",
      " |-- <lambda>(user_id): string (nullable = true)\n",
      " |-- gender0: string (nullable = true)\n",
      " |-- age0: string (nullable = true)\n",
      " |-- occupation0: string (nullable = true)\n",
      " |-- star0: string (nullable = true)\n",
      " |-- <lambda>(realtime): string (nullable = true)\n",
      " |-- len_predict_category_property: string (nullable = true)\n",
      " |-- context_page0: string (nullable = true)\n",
      " |-- <lambda>(shop_id): string (nullable = true)\n",
      " |-- shop_score_delivery0: string (nullable = true)\n",
      "\n",
      "###############  Cleaning & Encoding categorical data to one-hot expression   ################\n",
      "###############  item  ###############\n",
      "###############  user  ###############\n",
      "user 0,1 feature\n",
      "###############  context  ###############\n",
      "root\n",
      " |-- instance_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- item_brand_id: string (nullable = true)\n",
      " |-- item_city_id: string (nullable = true)\n",
      " |-- item_price_level: string (nullable = true)\n",
      " |-- item_sales_level: string (nullable = true)\n",
      " |-- item_collected_level: string (nullable = true)\n",
      " |-- item_pv_level: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_gender_id: string (nullable = true)\n",
      " |-- user_age_level: string (nullable = true)\n",
      " |-- user_occupation_id: string (nullable = true)\n",
      " |-- user_star_level: string (nullable = true)\n",
      " |-- context_id: string (nullable = true)\n",
      " |-- context_timestamp: string (nullable = true)\n",
      " |-- context_page_id: string (nullable = true)\n",
      " |-- predict_category_property: string (nullable = true)\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- shop_review_num_level: string (nullable = true)\n",
      " |-- shop_review_positive_rate: string (nullable = true)\n",
      " |-- shop_star_level: string (nullable = true)\n",
      " |-- shop_score_service: string (nullable = true)\n",
      " |-- shop_score_delivery: string (nullable = true)\n",
      " |-- shop_score_description: string (nullable = true)\n",
      " |-- category_0: string (nullable = true)\n",
      " |-- category_1: string (nullable = true)\n",
      " |-- category_2: string (nullable = true)\n",
      " |-- property_0: string (nullable = true)\n",
      " |-- property_1: string (nullable = true)\n",
      " |-- property_2: string (nullable = true)\n",
      " |-- realtime: string (nullable = true)\n",
      " |-- predict_category_0: string (nullable = true)\n",
      " |-- predict_category_1: string (nullable = true)\n",
      " |-- predict_category_2: string (nullable = true)\n",
      " |-- <lambda>(user_id): string (nullable = true)\n",
      " |-- gender0: string (nullable = true)\n",
      " |-- age0: string (nullable = true)\n",
      " |-- occupation0: string (nullable = true)\n",
      " |-- star0: string (nullable = true)\n",
      " |-- <lambda>(realtime): string (nullable = true)\n",
      "\n",
      "context 0,1 feature\n",
      "###############  shop  ###############\n",
      "root\n",
      " |-- instance_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- item_brand_id: string (nullable = true)\n",
      " |-- item_city_id: string (nullable = true)\n",
      " |-- item_price_level: string (nullable = true)\n",
      " |-- item_sales_level: string (nullable = true)\n",
      " |-- item_collected_level: string (nullable = true)\n",
      " |-- item_pv_level: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_gender_id: string (nullable = true)\n",
      " |-- user_age_level: string (nullable = true)\n",
      " |-- user_occupation_id: string (nullable = true)\n",
      " |-- user_star_level: string (nullable = true)\n",
      " |-- context_id: string (nullable = true)\n",
      " |-- context_timestamp: string (nullable = true)\n",
      " |-- context_page_id: string (nullable = true)\n",
      " |-- predict_category_property: string (nullable = true)\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- shop_review_num_level: string (nullable = true)\n",
      " |-- shop_review_positive_rate: string (nullable = true)\n",
      " |-- shop_star_level: string (nullable = true)\n",
      " |-- shop_score_service: string (nullable = true)\n",
      " |-- shop_score_delivery: string (nullable = true)\n",
      " |-- shop_score_description: string (nullable = true)\n",
      " |-- category_0: string (nullable = true)\n",
      " |-- category_1: string (nullable = true)\n",
      " |-- category_2: string (nullable = true)\n",
      " |-- property_0: string (nullable = true)\n",
      " |-- property_1: string (nullable = true)\n",
      " |-- property_2: string (nullable = true)\n",
      " |-- realtime: string (nullable = true)\n",
      " |-- predict_category_0: string (nullable = true)\n",
      " |-- predict_category_1: string (nullable = true)\n",
      " |-- predict_category_2: string (nullable = true)\n",
      " |-- <lambda>(user_id): string (nullable = true)\n",
      " |-- gender0: string (nullable = true)\n",
      " |-- age0: string (nullable = true)\n",
      " |-- occupation0: string (nullable = true)\n",
      " |-- star0: string (nullable = true)\n",
      " |-- <lambda>(realtime): string (nullable = true)\n",
      " |-- len_predict_category_property: string (nullable = true)\n",
      " |-- context_page0: string (nullable = true)\n",
      " |-- <lambda>(shop_id): string (nullable = true)\n",
      " |-- shop_score_delivery0: string (nullable = true)\n",
      "\n",
      "###############  Extracting Training Features   ################\n",
      "['instance_id', 'item_id', 'item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level', 'user_id', 'user_gender_id', 'user_age_level', 'user_occupation_id', 'user_star_level', 'context_id', 'context_timestamp', 'context_page_id', 'predict_category_property', 'shop_id', 'shop_review_num_level', 'shop_review_positive_rate', 'shop_star_level', 'shop_score_service', 'shop_score_delivery', 'shop_score_description', 'is_trade', 'category_0', 'category_1', 'category_2', 'property_0', 'property_1', 'property_2', 'realtime', 'predict_category_0', 'predict_category_1', 'predict_category_2', '<lambda>(user_id)', 'gender0', 'age0', 'occupation0', 'star0', '<lambda>(realtime)', 'len_predict_category_property', 'context_page0', '<lambda>(shop_id)', 'shop_score_delivery0']\n",
      "一个item在同类商品中item_price_level的归一化分数…(在同类商品价格中的分布)(查询词的预测cat0_1_2相同 if( != ))\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3566.describe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 13.0 failed 1 times, most recent failure: Lost task 18.0 in stage 13.0 (TID 27, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-19-e10f23bdaf6b>\", line 30, in <lambda>\n  File \"/home/vchim/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py\", line 111, in fit_transform\n    y = column_or_1d(y, warn=True)\n  File \"/home/vchim/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py\", line 614, in column_or_1d\n    raise ValueError(\"bad input shape {0}\".format(shape))\nValueError: bad input shape ()\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:266)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$.prepareShuffleDependency(ShuffleExchange.scala:221)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:244)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:365)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:75)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.doExecute(SortAggregateExec.scala:75)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:75)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.doExecute(SortAggregateExec.scala:75)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:311)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2160)\n\tat org.apache.spark.sql.Dataset$$anonfun$describe$1.apply(Dataset.scala:2126)\n\tat org.apache.spark.sql.Dataset$$anonfun$describe$1.apply(Dataset.scala:2108)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2872)\n\tat org.apache.spark.sql.Dataset.describe(Dataset.scala:2108)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-19-e10f23bdaf6b>\", line 30, in <lambda>\n  File \"/home/vchim/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py\", line 111, in fit_transform\n    y = column_or_1d(y, warn=True)\n  File \"/home/vchim/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py\", line 614, in column_or_1d\n    raise ValueError(\"bad input shape {0}\".format(shape))\nValueError: bad input shape ()\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-70aea869ed85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-4e3dcb3b9e51>\u001b[0m in \u001b[0;36mreadData\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#all_data = featureExtraction(pd.concat([train_data, test_data]).reset_index(drop=True))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"is_trade\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureExtraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-e4552180e52f>\u001b[0m in \u001b[0;36mfeatureExtraction\u001b[0;34m(raw_data)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mraw_data_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'counts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mraw_data_joined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'predict_category_0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict_category_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict_category_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mitem_norm_price_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprice_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_joined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m#raw_data_g = raw_data.groupby('predict_category_0', 'predict_category_1', 'predict_category_2').agg(min('item_price_level'), avg('item_price_level')).na.fill(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#raw_data_g.select('min(item_price_level)', 'avg(item_price_level)')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-b3de6f63d20a>\u001b[0m in \u001b[0;36mprice_normalization\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprice_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#df['item_norm_price_score'] = (df['item_price_level'] - df['item_price_level'].min()) / df['item_price_level'].mean()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'item_price_level'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#df1.select('min(item_price_level)')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#df = df.select('*',((df['item_price_level']-df1['min(item_price_level)'])/df1['avg(item_price_level)']).alias('item_norm_price_score'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vchim/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vchim/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3566.describe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 13.0 failed 1 times, most recent failure: Lost task 18.0 in stage 13.0 (TID 27, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-19-e10f23bdaf6b>\", line 30, in <lambda>\n  File \"/home/vchim/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py\", line 111, in fit_transform\n    y = column_or_1d(y, warn=True)\n  File \"/home/vchim/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py\", line 614, in column_or_1d\n    raise ValueError(\"bad input shape {0}\".format(shape))\nValueError: bad input shape ()\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:266)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$.prepareShuffleDependency(ShuffleExchange.scala:221)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:244)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:365)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:75)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.doExecute(SortAggregateExec.scala:75)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:75)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.doExecute(SortAggregateExec.scala:75)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:311)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2160)\n\tat org.apache.spark.sql.Dataset$$anonfun$describe$1.apply(Dataset.scala:2126)\n\tat org.apache.spark.sql.Dataset$$anonfun$describe$1.apply(Dataset.scala:2108)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2872)\n\tat org.apache.spark.sql.Dataset.describe(Dataset.scala:2108)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-19-e10f23bdaf6b>\", line 30, in <lambda>\n  File \"/home/vchim/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py\", line 111, in fit_transform\n    y = column_or_1d(y, warn=True)\n  File \"/home/vchim/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py\", line 614, in column_or_1d\n    raise ValueError(\"bad input shape {0}\".format(shape))\nValueError: bad input shape ()\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
