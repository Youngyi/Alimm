{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    print('[{name} done in {time.time() - start_time:.2f} s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# Following functions basically add groupby aggregation statistics, \n",
    "# then merge with original data frame\n",
    "def equalDepthNormal(data, feature_name=\"null\", n=200):\n",
    "    # 连续特征等频归一化 分为n个bucket(每个bucket样本数相同)\n",
    "    if (feature_name != \"null\"):\n",
    "        TOTAL_NUM = len(data)\n",
    "        data = (data-data.mean())/data.std()\n",
    "    return data\n",
    "\n",
    "def divide_vector(data):\n",
    "    del data['item_id']\n",
    "    del data['item_brand_id']\n",
    "    vector_list = ['item_id_vec', 'item_brand_id_vec']\n",
    "    for feat in vector_list:\n",
    "        for i in range(50):\n",
    "            data[feat+'_'+str(i)] = data[feat].apply(lambda x: x[i])\n",
    "        del data[feat]\n",
    "    return data\n",
    "\n",
    "def add_count(df, cols, cname, value):\n",
    "    df_count = pd.DataFrame(df.groupby(cols)[value].count()).reset_index()\n",
    "    df_count.columns = cols + [cname]\n",
    "    df = df.merge(df_count, on=cols, how='left')\n",
    "    del df_count\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def add_mean(df, cols, cname, value):\n",
    "    df_mean = pd.DataFrame(df.groupby(cols)[value].mean()).reset_index()\n",
    "    df_mean.columns = cols + [cname]\n",
    "    df = df.merge(df_mean, on=cols, how='left')\n",
    "    del df_mean\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def add_std(df, cols, cname, value):\n",
    "    df_std = pd.DataFrame(df.groupby(cols)[value].std()).reset_index()\n",
    "    df_std.columns = cols + [cname]\n",
    "    df = df.merge(df_std, on=cols, how='left')\n",
    "    del df_std\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def add_nunique(df, cols, cname, value):\n",
    "    df_nunique = pd.DataFrame(df.groupby(cols)[value].nunique()).reset_index()\n",
    "    df_nunique.columns = cols + [cname]\n",
    "    df = df.merge(df_nunique, on=cols, how='left')\n",
    "    del df_nunique\n",
    "    gc.collect()\n",
    "    return df\n",
    "    \n",
    "def add_cumcount(df, cols, cname):\n",
    "    df[cname] = df.groupby(cols).cumcount() + 1\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Following functions are dealing with predicted category and property:\n",
    "# calculate the number of correctly predicted category / property and\n",
    "# precision / recall\n",
    "\n",
    "def true_predict_count(true_lst, pred_lst):\n",
    "    items, cnt = true_lst.split(';'), 0\n",
    "    for i in pred_lst:\n",
    "        if i in items:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "def true_predict_precision(true_lst, pred_lst):\n",
    "    return true_predict_count(true_lst, pred_lst) / len(pred_lst)\n",
    "\n",
    "def true_predict_recall(true_lst, pred_lst):\n",
    "    return true_predict_count(true_lst, pred_lst) / len(true_lst.split(';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{name} done in {time.time() - start_time:.2f} s]\n"
     ]
    }
   ],
   "source": [
    "with timer('Read train and test'):\n",
    "    train_data = sc.textFile(\"./data/train.txt\")\n",
    "    test_data = sc.textFile(\"./data/test.txt\")\n",
    "    train_pairs = train_data.map(lambda m: tuple(m.split(' ')))\n",
    "    train_pairs.cache()\n",
    "    test_pairs = test_data.map(lambda m: tuple(m.split(' ')))\n",
    "    test_pairs.cache()\n",
    "    # Create the DataFrame from an RDD of tuples\n",
    "    train_header = ['instance_id', 'item_id', 'item_category_list', \\\n",
    "                    'item_property_list', 'item_brand_id', 'item_city_id', 'item_price_level', \\\n",
    "                    'item_sales_level', 'item_collected_level', 'item_pv_level', 'user_id', 'user_gender_id', \\\n",
    "                    'user_age_level', 'user_occupation_id', 'user_star_level', 'context_id', 'context_timestamp', \\\n",
    "                    'context_page_id', 'predict_category_property', 'shop_id', 'shop_review_num_level', \\\n",
    "                    'shop_review_positive_rate', 'shop_star_level', u'shop_score_service', 'shop_score_delivery', \\\n",
    "                    'shop_score_description', 'is_trade']\n",
    "    test_header = test_pairs.collect()[0]\n",
    "    train_df = spark.createDataFrame(train_pairs, train_header)\n",
    "    test_df = spark.createDataFrame(test_pairs, test_header)\n",
    "    train = train_df.filter(\"instance_id!='instance_id'\")  \n",
    "    test = test_df.filter(\"instance_id!='instance_id'\")\n",
    "    # sort by context_timestamp\n",
    "    train = train.orderBy('context_timestamp')\n",
    "    categorical_list = []\n",
    "    categorical_list.append('item_category')\n",
    "    split = udf(lambda x: x.split(\";\")[1])\n",
    "    split1 = udf(lambda x: x.split(\";\")[i] if len(x.split(\";\")) > i else \" \")\n",
    "    train = train.select('*', split(train['item_category_list']).alias('item_category'))\n",
    "    for i in range(1,3):\n",
    "        categorical_list.append('item_property_'+str(i))\n",
    "        train = train.select('*', split1(train['item_property_list']).alias('item_property_%d' % (i)))\n",
    "    le = LabelEncoder()\n",
    "    encode = udf(lambda l: le.fit_transform(l))\n",
    "    for i in categorical_list:\n",
    "        train = train.select('*', encode(train[i]))\n",
    "    train = train.drop('item_category','item_property_1','item_property_2')\n",
    "    # split train-validation with 9:1\n",
    "    train_ = train.limit(430324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----+\n",
      "|            shop_id|user_gender_id|count|\n",
      "+-------------------+--------------+-----+\n",
      "|6622647717976293081|             1|   25|\n",
      "|4261109238019410948|            -1|   10|\n",
      "|8083840712256217698|            -1|    4|\n",
      "|4132428120266854135|             0|   90|\n",
      "|6764523132891870704|            -1|    2|\n",
      "|1239481181711478081|             2|   17|\n",
      "|4791188645345883463|             1|   34|\n",
      "| 586859544414558419|             1|    1|\n",
      "|4215742678121737516|            -1|   68|\n",
      "|4456737157684315890|             0|   20|\n",
      "+-------------------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------------------+\n",
      "|avg(user_age_level_int)|\n",
      "+-----------------------+\n",
      "|      978.6990072444326|\n",
      "+-----------------------+\n",
      "\n",
      "root\n",
      " |-- avg(user_age_level_int): double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u\"grouping expressions sequence is empty, and '`context_page_id`' is not an aggregate function. Wrap '()' in windowing function(s) or wrap '`context_page_id`' in first() (or first_value) if you don't care which value you get.;;\\nAggregate [(cast(context_page_id#1082 as double) % cast(4000 as double)) AS (context_page_id % 4000)#1645]\\n+- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 6 more fields]\\n   +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 9 more fields]\\n      +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 8 more fields]\\n         +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 7 more fields]\\n            +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 6 more fields]\\n               +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 5 more fields]\\n                  +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 4 more fields]\\n                     +- Sort [context_timestamp#1081 ASC NULLS FIRST], true\\n                        +- Filter NOT (instance_id#1065 = instance_id)\\n                           +- LogicalRDD [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 3 more fields]\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-a9440e4240ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#df['context_page_id'] = df['context_page_id'] % 4000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context_page_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/vchim/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \"\"\"\n\u001b[0;32m-> 1165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vchim/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/group.pyc\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             jdf = self._jgd.agg(exprs[0]._jc,\n\u001b[0;32m---> 91\u001b[0;31m                                 _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vchim/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vchim/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"grouping expressions sequence is empty, and '`context_page_id`' is not an aggregate function. Wrap '()' in windowing function(s) or wrap '`context_page_id`' in first() (or first_value) if you don't care which value you get.;;\\nAggregate [(cast(context_page_id#1082 as double) % cast(4000 as double)) AS (context_page_id % 4000)#1645]\\n+- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 6 more fields]\\n   +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 9 more fields]\\n      +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 8 more fields]\\n         +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 7 more fields]\\n            +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 6 more fields]\\n               +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 5 more fields]\\n                  +- Project [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 4 more fields]\\n                     +- Sort [context_timestamp#1081 ASC NULLS FIRST], true\\n                        +- Filter NOT (instance_id#1065 = instance_id)\\n                           +- LogicalRDD [instance_id#1065, item_id#1066, item_category_list#1067, item_property_list#1068, item_brand_id#1069, item_city_id#1070, item_price_level#1071, item_sales_level#1072, item_collected_level#1073, item_pv_level#1074, user_id#1075, user_gender_id#1076, user_age_level#1077, user_occupation_id#1078, user_star_level#1079, context_id#1080, context_timestamp#1081, context_page_id#1082, predict_category_property#1083, shop_id#1084, shop_review_num_level#1085, shop_review_positive_rate#1086, shop_star_level#1087, shop_score_service#1088, ... 3 more fields]\\n\""
     ]
    }
   ],
   "source": [
    "with timer('Feature engineering'):\n",
    "    le = LabelEncoder()\n",
    "    df_full = [train, test]\n",
    "    df_full_processed = []\n",
    "    # get the customers (users) gender ratio for each shop\n",
    "    #df_shop_gender_ratio = train.groupby(['shop_id'])['user_gender_id']\\\n",
    "                            #.agg([lambda x: np.mean(x == 0)])\\\n",
    "                            #.rename(columns={'<lambda>': 'shop_user_gender_ratio'})\n",
    "    df_shop_gender_ratio = train.select('user_gender_id','shop_id').groupby('shop_id','user_gender_id').count()\n",
    "    df_shop_gender_ratio.show(10)\n",
    "    \n",
    "    # get the average age level of customers for each shop\n",
    "    df_shop_avg_age_level = train.select('user_age_level','shop_id').groupby('shop_id','user_age_level').avg()\n",
    "    df_shop_avg_age_level = df_shop_avg_age_level.select('*',df_shop_avg_age_level.user_age_level.cast('int').alias('user_age_level_int'))\n",
    "    df_shop_avg_age_level = df_shop_avg_age_level.agg(avg('user_age_level_int'))\n",
    "    df_shop_avg_age_level.show(5)\n",
    "    df_shop_avg_age_level.printSchema()\n",
    "    \n",
    "    for df in df_full:\n",
    "        # first item_category are same for all samples\n",
    "        \n",
    "        #df['context_page_id'] = df['context_page_id'] % 4000\n",
    "        df = df.agg(df['context_page_id'] % 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
